# -*- coding: utf-8 -*-
"""finetuning-GPT-2 (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nDGDdX2e-U6aMGYgjazdCIjWetvKlWQH

**OM NAMO NARAYANA**
"""


#############################
###   Instructions  #########
#############################
"""
pip install wandb
pip install transformers
wandb init

# log in key : 9d5065a2de6cbad8a4869ecf568cf7277676f833

"""


import os
import numpy as np
from tqdm import tqdm
from torch.utils.data import Dataset
import torch

import wandb
import re

import pandas as pd

from transformers import AutoTokenizer, AutoModelForCausalLM

from torch import nn
import torch.nn.functional as F


import torch.optim as optim

from datetime import datetime

import warnings


warnings.filterwarnings("ignore", category=DeprecationWarning)


# !pip install wandb
# !wandb login
# log in key : 9d5065a2de6cbad8a4869ecf568cf7277676f833

wandb.init(project="finetuning-GPT-2", entity="team-knitts")


root_path = 'H:/sem8/nlp/dataset/train/train/'
text_file_names = os.listdir(root_path)
dataset = []

for file_name in tqdm(text_file_names):
  with open(root_path + file_name, 'r', encoding="utf8") as f:
    dataset.append([f.read()])


dataset_processed = []
for tdata in tqdm(dataset):
  tdata = re.split('<?doc .*>|\n', tdata[0])[1:]
  dataset_processed.append(list(filter(None, [tdata_.strip('</doc>\n') for tdata_ in tdata])))
print(np.array(dataset_processed).shape)

dataset_combined = []
for data in dataset_processed:
  dataset_combined += data
del dataset_processed

# comment for training
# dataset_combined = dataset_combined[:100] # uncomment for testing



def corrupt_dataset(data):
  x = np.random.randint(2, size=1)[0]
  l = len(data) 
  s = int(len(data)/10)
  new_data = data
  if(x):
    label = 1
    places = [0] + list(np.random.randint(1, l-1, size=s))
    places.sort()

    ### logic to be optimized
    for i in range(len(places)-1):
      new_data += data[places[i]:places[i+1]-1]
    # new_data = data[:places[0]-1] + data[places[0] : places[1]-1] + data[places[1]:]
  else:
    places = [-1, -1]
    label = 0
  return {'data' : new_data, 'label' : label, 'places' : places }

corrupted_dataset = pd.DataFrame(list(map(corrupt_dataset, tqdm(dataset_combined))))
del corrupted_dataset['places']
pd_dataset = pd.DataFrame(corrupted_dataset)
del corrupt_dataset
del dataset_combined
pd_dataset.head()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
torch.cuda.empty_cache()
print('Using device:', device)



tokenizer = AutoTokenizer.from_pretrained("abinayam/gpt-2-tamil")
tokenizer.pad_token = 295 # to be changed

config = {
    'learning_rate' : 1e-3, # to be experimented
    'batch_size' : 3, # to be changed [crashing if high]
    'epochs' : 10,
    'betas': [0.9, 0.999]
}

wandb.config = config

from torch.utils.data import Dataset

class TamilDataset(Dataset):
    def __init__(self, dataset, target):
        self.dataset = dataset
        self.target = target

    def __len__(self):
        return len(self.dataset)


    def __getitem__(self, idx):
        # print(idx)
        batch = tokenizer(self.dataset[idx], truncation=True, max_length=1024, padding='max_length', return_tensors='pt')
        return {'data': batch['input_ids'].to(device), 'target': torch.tensor(np.array(self.target[idx], dtype=np.float32)).to(device)}

from torch.utils.data import DataLoader


temp = TamilDataset(list(pd_dataset['data']), list(pd_dataset['label']))
train_dataloader = DataLoader(temp, batch_size=config['batch_size'], shuffle=True)
batch = next(iter(train_dataloader))
# print(batch['data'].shape, batch['target'].shape)
# output = model(batch['data'])
# print(output.logits.shape)


class CustomGPTModel(nn.Module):
    def __init__(self):
          super(CustomGPTModel, self).__init__()
          self.model = AutoModelForCausalLM.from_pretrained("abinayam/gpt-2-tamil")
          self.model.to(device)
          ### To be optimized
          self.linear1 = nn.Linear(50257, 1024)
          self.linear2 = nn.Linear(1024, 512)
          self.linear3 = nn.Linear(512, 2)
          # self.linear4 = nn.Linear(256, 128)
          # self.linear5 = nn.Linear(128, 12)
          # self.linear6 = nn.Linear(12, 2)
          # self.sigmoid = nn.Sigmoid()

    def forward(self, batch):
          sequence_output= self.model(batch)

          logits = sequence_output.logits
          logits.to(device)

          # sequence_output has the following shape: (batch_size, sequence_length, 768)
          linear1_output = F.relu(self.linear1(sequence_output.logits[:,:,0,:].view(-1,50257))) ## extract the 1st token's embeddings

          linear2_output = F.relu(self.linear2(linear1_output))
          # linear3_output = F.relu(self.linear3(linear2_output))
          # linear4_output = F.relu(self.linear4(linear3_output))
          # linear5_output = F.relu(self.linear5(linear4_output))
          linear3_output = self.linear3(linear2_output)
          sigmoid_output = F.log_softmax(linear3_output)

          return sigmoid_output


# model_dir = '/content/drive/My Drive/model/'
model_dir = 'H:/sem8/nlp/implementation/checkpoint/models/'
load_model = False

try:
  if(load_model == False):
    raise Exception("don't load model - exception called")
  model = CustomGPTModel()
  optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], betas=config['betas'], eps=1e-08, weight_decay=0, amsgrad=False)
  latest_model_file = model_dir + sorted(os.listdir(model_dir))[-1]
  print('loading from ',latest_model_file)
  checkpoint = torch.load(latest_model_file)
  print('checkpoint loaded...')
  model.load_state_dict(checkpoint['model_state_dict'])
  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
  # epoch = checkpoint['epoch']
  print('model loaded...')
except: 
  print("can't load model...")
  model = CustomGPTModel()
  optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], betas=config['betas'], eps=1e-08, weight_decay=0, amsgrad=False)
  # criterion = nn.BCELoss()

criterion = nn.NLLLoss()
model.to(device)

epochs = config['epochs']


step = 0
for epoch in range(epochs):
  for t, batch in enumerate(train_dataloader):
    step += 1
    data = batch['data']
    targets = batch['target'] 
    
    optimizer.zero_grad()   
    outputs = model.forward(data)

    del data

    outputs = outputs.squeeze(-1)
    loss = criterion(outputs.to(device), targets.type(torch.LongTensor).to(device))
    wandb.log({"loss": loss.item()}, step=step)
    wandb.watch(model, log_freq = 20)
    if(epoch % 2 == 0 and t == 0 and epoch > 0):
      print('epoch:%d t:%d loss:%.2f'%(epoch, t, loss.item()))
      wandb.log({'targets': targets.cpu(), 'outputs': outputs.cpu()})
      print({'targets': targets.cpu(), 'outputs': outputs.cpu()})
      now = datetime.now() # current date and time
      date_time = now.strftime("%Y_%m_%d_%H_%M_%S")


      torch.save({
          'epoch' : epoch,
          'model_state_dict' : model.state_dict(),
          'optimizer_state_dict' : optimizer.state_dict(),
          'loss' : criterion,
      }, 'H:/sem8/nlp/implementation/checkpoint/models/' + 'model' + date_time + '.pth')

    

    loss.backward()
    optimizer.step()

    del outputs
    del targets


